#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------
X_bg_path = ifdef($X_bg_path, "census_xTrain.csv")
B_path    = ifdef($B_path, "census_bias.csv")
metadata_path=ifdef($metadata_path, "census_dummycoding_partitions.csv")

X_bg = read(data_dir+X_bg_path)
B = read(data_dir+B_path)
partitions=read(data_dir+metadata_path)

[row_phis, expected] = shapley_permutations_by_row(
  model_function="shap_l2svmPredict_custom",
  model_args=list(B=B),
  x_multirow=X_bg[500:600],
  X_bg=X_bg,
  n_permutations=3,
  integration_samples=100,
  remove_non_var=0,
  partitions=as.matrix(-1)
)

print("RowSums:\n"+toString(rowSums(row_phis)))

# Computes shapley values for multiple instances in parallel. This is the best approach for large datasets
# The resulting matrix phis holds the shapley values for each feature in the column given by the index of the feature in the sample.
#
# This execution policy first creates two large matrices for masks and masked background data for all permutations and
# then runs in paralell on all instances in x.
# While the prepared matrices can become very large (2 * #features * #permuations * #integration_samples * #features),
# the preparation of a row for the model call breaks down to a single element-wise multiplication of this mask with the row and
# an addition to the masked background data, since masks can be reused for each instance. It also benefits from a few less calls
# during the preparation, since all permuations can be prepared at once.
# If the number of permutations and integration samples is reasonably low and the memory of teh system is large enough,
# this allows for very fast preparation of samples.
#
# INPUT:
# ---------------------------------------------------------------------------------------
# model_function  The function of the model to be evaluated as a String. This function has to take a matrix of samples
#                 and return a vector of predictions.
#                 It might be usefull to wrap the model into a function the takes and returns the desired shapes and
#                 use this wrapper here.
# model_args      Arguments in order for the model, if desired. This will be prepended by the created instances-matrix.
# x_multirow      Multiple instances as rows for which to compute the shapley values.
# X_bg            The background dataset from which to pull the random samples to perform Monte Carlo integration.
# n_permutations  The number of permutaions. Defaults to 10. Theoretical 1 should already be enough for models with up
#                 to second order interaction effects.
# integration_samples Number of samples from X_bg used for marginalization.
# remove_non_var  If set, for every instance the varaince of each feature is checked against this feature in the
#                 background data. If it does not change, we do not run any model cals for it.
# seed            A seed, in case the sampling has to be deterministic.
# verbose         A boolean to enable logging of each step of the function.
# ---------------------------------------------------------------------------------------
#
# OUTPUT:
# -----------------------------------------------------------------------------
# S              Matrix holding the shapley values along the cols, one row per instance.
# expected       Double holding the average prediction of all instances.
# -----------------------------------------------------------------------------
shapley_permutations_by_row = function(String model_function, list[unknown] model_args, Matrix[Double] x_multirow,
  Matrix[Double] X_bg, Integer n_permutations = 10, Integer integration_samples = 100, Integer remove_non_var=0,
  Matrix[Double] partitions=as.matrix(-1), Integer seed = -1, Integer verbose = 1)
return (Matrix[Double] row_phis, Double expected){
  print(">>> DUMMY RUN! DOES NOT COMPUTE CORRECT SHAP VALUES <<<")
  u_vprint("--> Permutation Explainer (by-row, lean) for "+nrow(x_multirow)+" rows.", verbose)
  u_vprint("Number of Features: "+ncol(x_multirow), verbose )
  total_preds=ncol(x_multirow)*2*n_permutations*integration_samples*nrow(x_multirow)
  u_vprint("Number of predictions: "+toString(total_preds)+" in "+nrow(x_multirow)+
  " parallel cals.", verbose )

  #start with all features
  features=u_range(1, ncol(x_multirow))

  if(sum(partitions) != -1){
    if(remove_non_var != 0){
      stop("shapley_permutations_by_row:ERROR: Can't use n_non_varying_inds and partitions at the same time.")
    }
    features=removePartitionsFromFeatures(features, partitions)
    reduced_total_preds=ncol(features)*2*n_permutations*integration_samples*nrow(x_multirow)
    u_vprint("Using Partitions reduces number of features to "+ncol(features)+".", verbose )
    u_vprint("Total number of predictions reduced by "+(total_preds-reduced_total_preds)/total_preds+" to "+reduced_total_preds+".", verbose )
  }

  #important lengths and offsets
  total_features = ncol(x_multirow)
  perm_length = ncol(features)
  full_mask_offset = perm_length * 2 * integration_samples
  n_partition_features = total_features - perm_length

  #sample from X_bg
  u_vprint("-> Sampling from X_bg", verbose )
  # dummy approx of sample_with_potential_replace(X_bg=X_bg, samples=integration_samples, seed=seed )
  X_bg_samples = X_bg[1:integration_samples]
  row_phis     = matrix(0, rows=nrow(x_multirow), cols=total_features)
  expected_m   = matrix(0, rows=nrow(x_multirow), cols=1)

  #stats
  non_var_inds_stats = matrix(0, rows=nrow(x_multirow), cols=2)


  #prepare masks for all permutations, since it stays the same for every row
  u_vprint("-> Preparing reusable masks.", verbose )

  permutations    = matrix(0, rows=n_permutations, cols=perm_length)
  masks_for_permutations = matrix(0, rows=perm_length*2*n_permutations*integration_samples, cols=total_features)
  #masked_bg_for_permutations = matrix(0, rows=perm_length*2*n_permutations*integration_samples, cols=total_features)

  print(toString(features))
  print(ncol(features))
  for (i in 1:n_permutations, check=0){
    permutations[i] = features
  }
  for (i in 1:n_permutations, check=0){
    perm_mask = prepare_mask_for_permutation(permutation=permutations[i], partitions=partitions)
    offset_masks = (i-1) * full_mask_offset + 1
    masks_for_permutations[offset_masks:offset_masks+full_mask_offset-1]=prepare_full_mask(perm_mask, integration_samples)
  }

  #replicate background and mask it, since it also can stay the same for every row
  masked_bg_for_permutations = prepare_masked_X_bg(masks_for_permutations, X_bg_samples)
  u_vprint("-> Computing phis in parallel.", verbose )

  #use below parfor to force in spark
  #parfor (i in 1:nrow(x_multirow), opt=CONSTRAINED, mode=REMOTE_SPARK ){

  for (i in 1:nrow(x_multirow)){

    non_var_inds = as.matrix(-1)
    i_masks_for_permutations = masks_for_permutations
    i_masked_bg_for_permutations = masked_bg_for_permutations


    #apply masks and bg data for all permutations at once
    X_test = i_masked_bg_for_permutations + (i_masks_for_permutations * x_multirow[i])

    #generate args for call to model
    X_arg = append(list(X=X_test), model_args)

    #call model
    P = eval(model_function, X_arg)
    #compute means, deviding n_rows by integration_samples
    P = compute_means_from_predictions(P=P, integration_samples=integration_samples)

    #compute phis
    [phis, e] = compute_phis_from_prediction_means(P=P, permutations=permutations, non_var_inds=non_var_inds, n_partition_features=n_partition_features)
    expected_m[i] = e
    #compute phis for this row from all permutations
    row_phis[i] = t(phis)
  }
  #compute expected of model from all rows
  expected = mean(expected_m)
}


removePartitionsFromFeatures = function(Matrix[Double] features, Matrix[Double] partitions)
return (Matrix[Double] short_features){
  #remove from features
  rm_mask = matrix(0, rows=1, cols=ncol(features))
  for (i in 1:ncol(partitions)){
    part_start = as.scalar(partitions[1,i])
    part_end   = as.scalar(partitions[2,i])
    #include part_start as representative of partition
    rm_mask = rm_mask + (features > part_start) * (features <= part_end)
  }
  short_features = removeEmpty(target=features, margin="cols", select=!rm_mask)
}

repeatMatrix = function(Matrix[Double] m, Integer n_times)
return(Matrix[Double] m){
n_rows=nrow(m)
n_cols=ncol(m)
#reshape to row vector
m = matrix(m, rows=1, cols=length(m))
#broadcast
m = matrix(1, rows=n_times, cols=1) * m
#reshape to get matrix
m = matrix(m, rows=n_rows*n_times, cols=n_cols)
}

prepare_mask_for_permutation = function(Matrix[Double] permutation, Integer n_non_varying_inds=0,
       Matrix[Double] partitions=as.matrix(-1))
return (Matrix[Double] masks){
  if(sum(partitions)!=-1){
    #can't use n_non_varying_inds and partitions at the same time
    if(n_non_varying_inds > 0){
      stop("prepare_mask_for_permutation:ERROR: Can't use n_non_varying_inds and partitions at the same time.")
    }
    #number of features not in permutation is diff between start and end of partitions, since first feature remains in permutation
    skip_inds = partitions[2,] - partitions[1,]

    #skip these inds by treating them as non varying
    n_non_varying_inds = sum(skip_inds)
  }

  #total number of features
  perm_len = ncol(permutation)+n_non_varying_inds
  if(n_non_varying_inds > 0){
    #prep full constructor with placeholders
    mask_constructor = matrix(perm_len+1, rows=1, cols = perm_len)
    mask_constructor[1,perm_len-ncol(permutation)+1:perm_len] = permutation
  }else{
    mask_constructor=permutation
  }

  perm_cols = ncol(mask_constructor)

  # we compute mask on reverse permutation wnd reverse it later to get desired shape

  # create row indicator vector ctable
  perm_mask_rows = seq(1,perm_cols)
  #TODO: col-vector and matrix mult?
  perm_mask_rows = matrix(1, rows=perm_cols, cols=perm_cols) * perm_mask_rows
  perm_mask_rows = lower.tri(target=perm_mask_rows, diag=TRUE, values=TRUE)
  perm_mask_rows = removeEmpty(target=matrix(perm_mask_rows, rows=1, cols=length(perm_mask_rows)), margin="cols")

  # create column indicator for ctable
  rev_permutation = t(rev(t(mask_constructor)))
  #TODO: col-vector and matrix mult?
  perm_mask_cols = matrix(1, rows=perm_cols, cols=perm_cols) * rev_permutation
  perm_mask_cols = lower.tri(target=perm_mask_cols, diag=TRUE, values=TRUE)
  perm_mask_cols = removeEmpty(target = matrix(perm_mask_cols, cols=length(perm_mask_cols), rows=1), margin="cols")

  #ctable
  masks = table(perm_mask_rows, perm_mask_cols, perm_len, perm_len)
  if(n_non_varying_inds > 0){
    #truncate non varying rows
    masks = masks[1:ncol(permutation)]

    #replicate mask from first feature of each partionton to entire partitions
    if(sum(partitions)!=-1){
      for ( i in 1:ncol(partitions) ){
        p_start = as.scalar(partitions[1,i])
        p_end   = as.scalar(partitions[2,i])
        test = masks[,p_start] %*% matrix(1, rows=1, cols=p_end-p_start)
        masks[,p_start+1:p_end] = test
      }
    }
  }

  # add inverted mask and revert order for desired shape for forward and backward pass
  masks = rev(rbind(masks, !masks))
}

prepare_full_mask = function(Matrix[Double] mask, Integer n_integration_samples)
return (Matrix[Double] x_mask_full){
  x_mask_full = repeatRows(mask,n_integration_samples)
}

prepare_masked_X_bg = function(Matrix[Double] x_mask_full, Matrix[Double] X_bg_samples)
return (Matrix[Double] masked_X_bg){
  #Repeat background once for every row in original mask.
  #Since x_mask_full was already replicated row-wise by the number of rows in X_bg_samples, we devide by it.
  masked_X_bg = repeatMatrix(X_bg_samples, nrow(x_mask_full)/nrow(X_bg_samples))
  masked_X_bg = masked_X_bg * !x_mask_full
}

compute_means_from_predictions = function(Matrix[Double] P, Integer integration_samples)
return (Matrix[Double] P_means){
  n_features = nrow(P)/integration_samples

  #transpose and reshape to concat all values of same type
  # TODO: unneccessary for vectors, only t() would be needed
  P = matrix(t(P), cols=1, rows=length(P))

  #reshape, so all predictions from one batch are in one row
  P = matrix(P, cols=integration_samples, rows=length(P)/integration_samples)

  #compute row means
  P_means = rowMeans(P)

  # reshape and transpose to get back to input dimensions
  P_means = matrix(P_means, rows=n_features, cols=length(P_means)/n_features)
}

compute_phis_from_prediction_means = function(Matrix[Double] P, Matrix[Double] permutations,
  Matrix[Double] non_var_inds=as.matrix(-1), Integer n_partition_features = 0)
return(Matrix[Double] phis, Double expected){
  perm_len=ncol(permutations)
  n_non_var_inds = 0
  partial_permutations = permutations

  if(sum(non_var_inds)>0){
    n_non_var_inds = ncol(non_var_inds)
    #flatten perms to remove from all perms at once
    perms_flattened = matrix(permutations, rows=length(permutations), cols=1)
    rem_selector = outer(perms_flattened, non_var_inds, "==")
    rem_selector = rowSums(rem_selector)
    partial_permutations = removeEmpty(target=perms_flattened, select=!rem_selector, margin="rows")
    #reshape
    partial_permutations = matrix(partial_permutations, rows=perm_len-n_non_var_inds, cols=nrow(permutations))
    perm_len = perm_len-n_non_var_inds
  }

  #reshape P to get one col per permutation
  P_perm = matrix(P, rows=2*perm_len, cols=nrow(permutations), byrow=FALSE)

  #compute forward results (inds_with - inds_without)
  forward_phis = P_perm[2:perm_len+1] - P_perm[1:perm_len]

  #compute backwards results reverse of (inds_with - inds_without) for first n-1 phis
  # --> breaks, if only on feature changes
  backward_phis = P_perm[perm_len+1:2*perm_len-1] - P_perm[perm_len+2:2*perm_len]
  #compute last backward, because we reuse result from index 1 to have less model calls
  backward_phis = rbind(backward_phis, P_perm[2*perm_len] - P_perm[1])

  ###
  #avg forward and backward
  forward_phis = matrix(forward_phis, rows=length(forward_phis), cols=1, byrow=FALSE)
  backward_phis = matrix(backward_phis, rows=length(backward_phis), cols=1, byrow=FALSE)
  avg_phis = (forward_phis + backward_phis) / 2

  #aggregate to get only one phi per feature (and implicitly add zeros for non var inds)
  perms_flattened = matrix(partial_permutations, rows=length(partial_permutations), cols=1)
  phis = aggregate(target=avg_phis, groups=perms_flattened, fn="mean", ngroups=ncol(permutations)+n_partition_features)

  #get expected from first row
  expected=mean(P_perm[1])
}

repeatRows = function(Matrix[Double] m, Integer n_times)
return(Matrix[Double] m){
  #get indices for new rows (e.g. 1,1,1,2,2,2 for 2 rows, each replicated 3 times)
  indices = ceil(seq(1,nrow(m)*n_times,1) / n_times)
  #to one hot, so we get a replication matrix R
  R = toOneHot(indices, nrow(m))
  #matrix-mulitply to repeat rows
  m = R %*% m
}

shap_l2svmPredict_custom = function(Matrix[Double] X, Matrix[Double] B)
return( Matrix[Double] P){
[P, n] = l2svmPredict(X=X, W=B, verbose=FALSE)
}

#utils

u_vprint = function(String message, Boolean verbose){
  if(verbose){
    print("shapley::"+message)
  }
}

u_range = function(Integer start, Integer end)
return (Matrix[Double] range){
  range = t(cumsum(matrix(1, rows=end-start+1, cols=1)))
  range = range+start-1
}

u_shuffle = function(Matrix[Double] X)
return (Matrix[Double] X_shuffled){
  num_col = ncol(X)
  # Random vector used to shuffle the dataset
  y = rand(rows=nrow(X), cols=1, min=0, max=1, pdf="uniform")
  X = order(target = cbind(X, y), by = num_col + 1)
  X_shuffled = X[,1:num_col]
}